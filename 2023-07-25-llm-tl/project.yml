title: "Benchmarking Tagalog datasets on LLMs"
description: |
  Accompanying spaCy project for my blog post, [*Do large language models work on Tagalog?*](https://ljvmiranda921.github.io/notebook/2023/10/18/llm-tagalog/).
  Here, I used [spacy-llm](https://github.com/explosion/spacy-llm) to access different LLMs.
  I highly-recommend checking the [documentation](https://spacy.io/api/large-language-models) on how to use the framework.

# These are the directories that the project needs. The project CLI will make
# sure that they always exist.
directories:
  - "assets"
  - "assets/treebank"
  - "corpus"
  - "corpus/parser-ud/trg"
  - "corpus/parser-ud/ugnayan"
  - "configs"
  - "pipelines"
  - "scripts"
  - "metrics"

# Variables can be referenced across the project.yml using ${vars.var_name}
vars:
  lang: "tl"
  seed: 0

# Assets that should be downloaded or available in the directory. But the
# 'project assets' command still lets you verify that the checksums match.
assets:
  - dest: "assets/treebank/UD_Tagalog-Ugnayan/"
    description: "Treebank data for UD_Tagalog-Ugnayan. Originally sourced from *Parsing in the absence of related languages: Evaluating low-resource dependency parsers in Tagalog* by Aquino and de Leon (2020)."
    git:
      repo: "https://github.com/UniversalDependencies/UD_Tagalog-Ugnayan"
      branch: "master"
      path: ""
  - dest: "assets/treebank/UD_Tagalog-TRG/"
    description: "Treebank data for UD_Tagalog-TRG. Originally sourced from the thesis, *A treebank prototype for Tagalog*, at the University of TÃ¼bingen by Samson (2018)."
    git:
      repo: "https://github.com/UniversalDependencies/UD_Tagalog-TRG"
      branch: "master"
      path: ""
  - dest: "assets/hatespeech.zip"
    description: "Contains 10k tweets with 4.2k testing and validation data labeled as hate speech or non-hate speech (text categorization). Based on *Hate speech in Philippine election-related tweets: Automatic detection and classification using natural language processing* by Cabasag et al. (2019)"
    url: "https://s3.us-east-2.amazonaws.com/blaisecruz.com/datasets/hatenonhate/hatespeech_raw.zip"
  - dest: "assets/dengue.zip"
    description: "Contains tweets on dengue labeled with five different categories. Tweets can be categorized to multiple categories at the same time (multilabel text categorization). Based on *Monitoring dengue using Twitter and deep learning techniques* by Livelo and Cheng (2018)."
    url: "https://s3.us-east-2.amazonaws.com/blaisecruz.com/datasets/dengue/dengue_raw.zip"
  - dest: assets/calamancy_gold.tar.gz
    description: "Contains the annotated TLUnified corpora in spaCy format with PER, ORG, LOC as entity labels (named entity recognition). Annotated by three annotators with IAA (Cohen's Kappa) of 0.78. Corpora was based from *Improving Large-scale Language Models and Resources for Filipino* by Cruz and Cheng (2021)."
    url: "https://storage.googleapis.com/ljvmiranda/calamanCy/tl_tlunified_gold/v1.0/corpus.tar.gz"
  - dest: "scripts/process_dengue.py"
    description: "Processing script for the Dengue dataset"
    url: "https://github.com/ljvmiranda921/calamanCy/blob/master/report/benchmark/scripts/process_dengue.py"
  - dest: "scripts/process_hatespeech.py"
    description: "Processing script for the Hatespeech dataset"
    url: "https://github.com/ljvmiranda921/calamanCy/blob/master/report/benchmark/scripts/process_hatespeech.py"
  - dest: "scripts/split_treebank.py"
    description: "Processing script for the UD treebanks"
    url: "https://github.com/ljvmiranda921/calamanCy/blob/master/report/benchmark/scripts/split_treebank.py"

commands:
  - name: "process-datasets"
    help: "Process the datasets and convert them into spaCy format"
    script:
      # textcat: extract and process Hatespeech dataset
      - unzip -o assets/hatespeech.zip -d assets/
      - mv assets/hatespeech/valid.csv assets/hatespeech/dev.csv
      - python -m scripts.process_hatespeech assets/hatespeech/ corpus/textcat-hatespeech/
      # textcat_multilabel: extract and process Dengue dataset
      - unzip -o assets/dengue.zip -d assets/
      - mv assets/dengue/valid.csv assets/dengue/dev.csv
      - python -m scripts.process_dengue assets/dengue/ corpus/textcat_multilabel-dengue/
      # ner: extract TLUnified-NER dataset.
      - mkdir -p corpus/ner-calamancy_gold/
      - tar -xzvf assets/calamancy_gold.tar.gz -C corpus/ner-calamancy_gold/
      # parser: convert treebank into spaCy format and then merge them
      - mkdir -p corpus/parser-ud/
      - python -m spacy convert assets/treebank/UD_Tagalog-Ugnayan/tl_ugnayan-ud-test.conllu assets/treebank --converter conllu --n-sents 1 --merge-subtokens
      - python -m scripts.split_treebank assets/treebank/tl_ugnayan-ud-test.spacy corpus/parser-ud/ugnayan/ --lang ${vars.lang} --train-size 0.9 --seed ${vars.seed}
      - python -m spacy convert assets/treebank/UD_Tagalog-TRG/tl_trg-ud-test.conllu assets/treebank --converter conllu --n-sents 1 --merge-subtokens
      - python -m scripts.split_treebank assets/treebank/tl_trg-ud-test.spacy corpus/parser-ud/trg/ --lang ${vars.lang} --train-size 0.9 --seed ${vars.seed}
    deps:
      - assets/hatespeech.zip
      - assets/dengue.zip
      - assets/calamancy_gold.tar.gz
      - assets/treebank/UD_Tagalog-Ugnayan/tl_ugnayan-ud-test.conllu
      - assets/treebank/UD_Tagalog-TRG/tl_trg-ud-test.conllu
      - scripts/process_dengue.py
      - scripts/process_hatespeech.py
      - scripts/split_treebank.py
    outputs:
      - corpus/textcat-hatespeech/train.spacy
      - corpus/textcat-hatespeech/dev.spacy
      - corpus/textcat-hatespeech/test.spacy
      - corpus/textcat_multilabel-dengue/train.spacy
      - corpus/textcat_multilabel-dengue/dev.spacy
      - corpus/textcat_multilabel-dengue/test.spacy
      - corpus/ner-calamancy_gold/train.spacy
      - corpus/ner-calamancy_gold/dev.spacy
      - corpus/ner-calamancy_gold/test.spacy
      - corpus/parser-ud/ugnayan/train.spacy
      - corpus/parser-ud/ugnayan/dev.spacy
      - corpus/parser-ud/trg/train.spacy
      - corpus/parser-ud/trg/dev.spacy
