title: "Benchmarking Tagalog datasets on LLMs"
description: |
  Accompanying spaCy project for my blog post, [*Do large language models work on Tagalog?*](https://ljvmiranda921.github.io/notebook/2023/10/18/llm-tagalog/).
  Here, I used [spacy-llm](https://github.com/explosion/spacy-llm) to access different LLMs.
  I highly-recommend checking the [documentation](https://spacy.io/api/large-language-models) on how to use the framework.

  > **Note**  
  For OpenAI, Anthropic, and Cohere, you need to set API keys in your env. Check [this page](https://spacy.io/api/large-language-models#api-keys) for more info. 

  You can run a specific pipeline via the `llm` workflow. 
  You need to pass a `vars.model_family` and a `vars.model_name`. 
  The model family is what you pass to the `@llm_models` parameter while the model name is the specific variant of that particular model.
  You can find more information in the [model documentation](https://spacy.io/api/large-language-models#models).

  For example, if you wish to evaluate on the `32k` version of GPT-4, then run the following:

  ```sh
  python -m spacy project run llm . --vars.model_family "spacy.GPT-4.v1" --vars.model_name "gpt-4-32k"
  ```

  You can also use the handy utility script to easily run multiple benchmarks:

  ```sh
  python -m scripts.benchmark all                  # Run all model configurations
  python -m scripts.benchmark gpt4 cohere          # Run OpenAI GPT-4 and Cohere only
  python -m scripts.benchmark all --ignore gpt4    # Run all except the GPT-4 config
  ```

  For more information, run:

  ```sh
  python -m scripts.benchmark --help
  ```

# These are the directories that the project needs. The project CLI will make
# sure that they always exist.
directories:
  - "assets"
  - "corpus"
  - "configs"
  - "pipelines"
  - "scripts"
  - "metrics"

# Variables can be referenced across the project.yml using ${vars.var_name}
vars:
  lang: tl
  seed: 0
  model_family: spacy.GPT-4.v1
  model_name: gpt-4
  cache_dir: cache
  remote_gcs_bucket: "ljvmiranda"
  batch_size: 8

remotes:
  # Create a service account, download a JSON key, and set the credentials path
  # to GOOGLE_APPLICATION_CREDENTIALS env variable
  gcs: "gs://${vars.remote_gcs_bucket}/experiments/tl_llm/"

# Assets that should be downloaded or available in the directory. But the
# 'project assets' command still lets you verify that the checksums match.
assets:
  - dest: "assets/hatespeech.zip"
    description: "Contains 10k tweets with 4.2k testing and validation data labeled as hate speech or non-hate speech (text categorization). Based on *Hate speech in Philippine election-related tweets: Automatic detection and classification using natural language processing* by Cabasag et al. (2019)"
    url: "https://s3.us-east-2.amazonaws.com/blaisecruz.com/datasets/hatenonhate/hatespeech_raw.zip"
  - dest: "assets/dengue.zip"
    description: "Contains tweets on dengue labeled with five different categories. Tweets can be categorized to multiple categories at the same time (multilabel text categorization). Based on *Monitoring dengue using Twitter and deep learning techniques* by Livelo and Cheng (2018)."
    url: "https://s3.us-east-2.amazonaws.com/blaisecruz.com/datasets/dengue/dengue_raw.zip"
  - dest: assets/calamancy_gold.tar.gz
    description: "Contains the annotated TLUnified corpora in spaCy format with PER, ORG, LOC as entity labels (named entity recognition). Annotated by three annotators with IAA (Cohen's Kappa) of 0.78. Corpora was based from *Improving Large-scale Language Models and Resources for Filipino* by Cruz and Cheng (2021)."
    url: "https://storage.googleapis.com/ljvmiranda/calamanCy/tl_tlunified_gold/v1.0/corpus.tar.gz"
  - dest: "scripts/process_dengue.py"
    description: "Processing script for the Dengue dataset"
    url: "https://github.com/ljvmiranda921/calamanCy/blob/master/report/benchmark/scripts/process_dengue.py"
  - dest: "scripts/process_hatespeech.py"
    description: "Processing script for the Hatespeech dataset"
    url: "https://github.com/ljvmiranda921/calamanCy/blob/master/report/benchmark/scripts/process_hatespeech.py"

# Workflows are sequences of commands (see below) executed in order. You can
# run them via "spacy project run [workflow]". If a commands's inputs/outputs
# haven't changed, it won't be re-run.
workflows:
  all:
    - process-datasets
    - ner
    - textcat
  llm:
    - ner
    - textcat

# Project commands, specified in a style similar to CI config files (e.g. Azure
# pipelines). The name is the command name that lets you trigger the command
# via "spacy project run [command] [path]". The help message is optional and
# shown when executing "spacy project run [optional command] [path] --help".
commands:
  - name: "process-datasets"
    help: "Process the datasets and convert them into spaCy format"
    script:
      # textcat: extract and process Hatespeech dataset
      - unzip -o assets/hatespeech.zip -d assets/
      - mv assets/hatespeech/valid.csv assets/hatespeech/dev.csv
      - python -m scripts.process_hatespeech assets/hatespeech/ corpus/textcat-hatespeech/
      # textcat_multilabel: extract and process Dengue dataset
      - unzip -o assets/dengue.zip -d assets/
      - mv assets/dengue/valid.csv assets/dengue/dev.csv
      - python -m scripts.process_dengue assets/dengue/ corpus/textcat_multilabel-dengue/
      # ner: extract TLUnified-NER dataset.
      - mkdir -p corpus/ner-calamancy_gold/
      - tar -xzvf assets/calamancy_gold.tar.gz -C corpus/ner-calamancy_gold/
    deps:
      - assets/hatespeech.zip
      - assets/dengue.zip
      - assets/calamancy_gold.tar.gz
      - scripts/process_dengue.py
      - scripts/process_hatespeech.py
    outputs:
      - corpus/textcat-hatespeech/train.spacy
      - corpus/textcat-hatespeech/dev.spacy
      - corpus/textcat-hatespeech/test.spacy
      - corpus/textcat_multilabel-dengue/train.spacy
      - corpus/textcat_multilabel-dengue/dev.spacy
      - corpus/textcat_multilabel-dengue/test.spacy
      - corpus/ner-calamancy_gold/train.spacy
      - corpus/ner-calamancy_gold/dev.spacy
      - corpus/ner-calamancy_gold/test.spacy

  - name: "ner"
    help: "Run an LLM pipeline on an NER task"
    script:
      - mkdir -p cache/${vars.model_name}/ner/
      - python -m spacy assemble configs/ner.cfg pipelines/${vars.model_name}-ner/ --model.family ${vars.model_family} --model.name ${vars.model_name} --model.cache_dir cache/${vars.model_name}/ner/ --nlp.batch_size ${vars.batch_size}
      - mkdir -p metrics/${vars.model_name}/ner/
      - python -m spacy benchmark accuracy pipelines/${vars.model_name}-ner/ corpus/ner-calamancy_gold/test.spacy --output metrics/${vars.model_name}/ner/results.json
    deps:
      - corpus/ner-calamancy_gold/test.spacy
    outputs:
      - cache/${vars.model_name}/ner/
      - metrics/${vars.model_name}/ner/results.json

  - name: "textcat"
    help: "Run an LLM pipeline on a TextCat task"
    script:
      - mkdir -p cache/${vars.model_name}/textcat_multilabel-dengue/
      - python -m spacy assemble configs/textcat_dengue.cfg pipelines/${vars.model_name}-textcat_multilabel-dengue/ --model.family ${vars.model_family} --model.name ${vars.model_name} --model.cache_dir cache/${vars.model_name}/textcat_multilabel-dengue/ --nlp.batch_size ${vars.batch_size}
      - mkdir -p metrics/${vars.model_name}/textcat_multilabel-dengue/
      - python -m spacy benchmark accuracy pipelines/${vars.model_name}-textcat_multilabel-dengue/ corpus/textcat_multilabel-dengue/test.spacy --output metrics/${vars.model_name}/textcat_multilabel-dengue/results.json
      - mkdir -p cache/${vars.model_name}/textcat-hatespeech/
      - python -m spacy assemble configs/textcat_hatespeech.cfg pipelines/${vars.model_name}-textcat-hatespeech/ --model.family ${vars.model_family} --model.name ${vars.model_name} --model.cache_dir cache/${vars.model_name}/textcat-hatespeech/ --nlp.batch_size ${vars.batch_size}
      - mkdir -p metrics/${vars.model_name}/textcat-hatespeech/
      - python -m spacy benchmark accuracy pipelines/${vars.model_name}-textcat-hatespeech/ corpus/textcat-hatespeech/test.spacy --output metrics/${vars.model_name}/textcat-hatespeech/results.json
    deps:
      - corpus/textcat-hatespeech/test.spacy
      - corpus/textcat_multilabel-dengue/test.spacy
    outputs:
      - cache/${vars.model_name}/textcat-hatespeech/
      - cache/${vars.model_name}/textcat_multilabel-dengue/
      - metrics/${vars.model_name}/textcat-hatespeech/results.json
      - metrics/${vars.model_name}/textcat_multilabel-dengue/results.json
