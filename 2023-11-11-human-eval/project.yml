title: "Am I smarter than a text generator?"
description: |
  Some Prodigy annotation tasks to try out some common LLM benchmarks
  First, you need to download the dataset and specify the annotation interface (choice/textbox) it will be converted into.

  ```sh
  weasel run setup . --vars.dataset piqa --vars.interface choice
  ```

  Then, you can run [Prodigy](https://prodigy.ai) to start annotating:

  ```sh
  weasel run annotate . --vars.dataset piqa --vars.interface choice
  ```

  This saves all your annotations to the `humaneval_{dataset}_{interface}`
  Prodigy dataset, which you can then export using [`prodigy db-out`](https://prodi.gy/docs/recipes#db-out) command.

vars:
  interface: "choice"
  dataset: "piqa"
  split: "validation"

# These are the directories that the project needs. The project CLI will make
# sure that they always exist.
directories:
  - "annotations"
  - "assets"
  - "corpus"
  - "scripts"

# Assets that should be downloaded or available in the directory. But the
# 'project assets' command still lets you verify that the checksums match.
assets:
  - dest: "annotations/piqa/humaneval_piqa_choice.jsonl"
    description: "PIQA Human annotations by Lj"
  - dest: "annotations/hellaswag/humaneval_hellaswag_choice.jsonl"
    description: "HellaSwag Human annotations by Lj"

# Project commands, specified in a style similar to CI config files (e.g. Azure
# pipelines). The name is the command name that lets you trigger the command
# via "spacy project run [command] [path]". The help message is optional and
# shown when executing "spacy project run [optional command] [path] --help".
commands:
  - name: "download"
    help: "Download dataset from HuggingFace and convert them into Prodigy format"
    script:
      - >-
        python -m scripts.download_dataset corpus/${vars.dataset}_${vars.interface}.jsonl 
        --dataset ${vars.dataset}
        --split ${vars.split}
        --interface ${vars.interface}
    outputs:
      - corpus/${vars.dataset}_${vars.interface}.jsonl

  - name: "annotate"
    help: "Annotate a dataset using Prodigy."
    script:
      - prodigy humaneval humaneval_${vars.dataset}_${vars.interface} corpus/${vars.dataset}_${vars.interface}.jsonl -v ${vars.interface} -F scripts/recipe_humaneval.py
    deps:
      - corpus/${vars.dataset}_${vars.interface}.jsonl

  - name: "export"
    help: "Export into JSONL format"
    script:
      - mkdir -p annotations/${vars.dataset}/
      - prodigy db-out humaneval_${vars.dataset}_${vars.interface} annotations/${vars.dataset}/
    outputs:
      - annotations/${vars.dataset}/humaneval_${vars.dataset}_${vars.interface}.jsonl

  - name: "evaluate-gold"
    help: "Evaluate against gold dataset"
    script:
      - >-
        python -m scripts.evaluate_gold ${vars.dataset}
        corpus/${vars.dataset}_${vars.interface}.jsonl
        annotations/${vars.dataset}/humaneval_${vars.dataset}_${vars.interface}.jsonl
    deps:
      - corpus/${vars.dataset}_${vars.interface}.jsonl
      - annotations/${vars.dataset}/humaneval_${vars.dataset}_${vars.interface}.jsonl
