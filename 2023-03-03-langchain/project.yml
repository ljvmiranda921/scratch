title: "Including annotation guidelines for argument mining annotation"
description: |
  Accompanying spaCy project for my blog post, [*GPT-3 for argument mining
  annotation*](https://ljvmiranda921.github.io/notebook/2023/05/03/annotation-guidlines-llm/),
  where I explored how we can include annotation guidelines for LLM-assisted
  annotation for complex tasks like argument mining. 

  I am using an argument mining dataset from the [UKP Sentential Argument Mining
  Corpus](https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/2345) ([Stab et
  al., 2018](https://aclanthology.org/D18-1402/)). Here, they have sentences on
  a variety of issues like cloning, minimum wage, abortion, with labels such as
  `NoArgument`, `Argument_For`, and `Argument_Against`. Note that you need to
  [send a request](https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/2345/restricted-resource?bitstreamId=90a1de18-7a2e-4706-89e6-cf8108cfd3e9)
  to the TU Datalib in order to access the corpus. Once you have the data, copy
  the `cloning.tsv` and `minimum_wage.tsv` into the `assets/` directory.

  ### Quick setup

  Make sure to [install Prodigy](https://prodi.gy/docs/install) as well as a few additional Python dependencies:

  ```bash
  python -m pip install prodigy -f https://XXXX-XXXX-XXXX-XXXX@download.prodi.gy
  python -m pip install -r requirements.txt
  ```

  With `XXXX-XXXX-XXXX-XXXX` being your personal Prodigy license key.

  Then, create a new API key from [openai.com](https://beta.openai.com/account/api-keys) or fetch an existing
  one. Record the secret key as well as the [organization key](https://beta.openai.com/account/org-settings)
  and make sure these are available as environmental variables. For instance, set them in a `.env` file in the
  root directory:

  ```
  OPENAI_ORG = "org-..."
  OPENAI_KEY = "sk-..."
  ```

# These are the directories that the project needs. The project CLI will make
# sure that they always exist.
directories:
  - "assets"
  - "assets/guidelines"
  - "corpus"
  - "corpus/jsonl"
  - "corpus/spacy"
  - "metrics"
  - "outputs"
  - "scripts"

# Variables can be referenced across the project.yml using ${vars.var_name}
vars:
  source_name: "minimum_wage"
  dataset: "train"
  annotation_guideline: "morante2020"
  config: "textcat.cfg"
  gpu_id: 0

# Assets that should be downloaded or available in the directory. But the
# 'project assets' command still lets you verify that the checksums match.
assets:
  - dest: "assets/guidelines/morante2020.pdf"
    url: "https://github.com/terne/Annotator-Bias-in-Argmin/blob/main/guidelines/guideline_one.pdf"
    description: >-
      Annotation guidelines from [Morante et al.
      (2020)](https://aclanthology.org/2020.lrec-1.611) for context-independent
      claim-like sentence detection. The original guidelines are from
      https://git.io/J1OKR and the IAA is measured by its token-level annotation
      F-score, which in this case is 42.4.
  - dest: "assets/guidelines/levy2018.pdf"
    url: "https://github.com/terne/Annotator-Bias-in-Argmin/blob/main/guidelines/guideline_two.pdf"
    description: >-
      Annotation guidelines from [Levy at al.
      (2018)](https://aclanthology.org/C18-1176/) for context-dependent
      claim-detection.  Here, the term claim refers to the "assertion the
      argument aims to prove" or simply, the conclusion. The IAA, using Cohen's
      kappa metric, is 0.58.
  - dest: "assets/guidelines/stab2018.pdf"
    url: "https://github.com/terne/Annotator-Bias-in-Argmin/blob/main/guidelines/guideline_three.pdf"
    description: >-
      Annotation guidelines from [Stab et al.
      (2018)](https://aclanthology.org/D18-1402/) for context-dependent claim
      and premise detection. The UKP dataset came from this work.  Sometimes, it
      contains statements of general topics that do not reflect a conclusion in
      itself. In the original paper, they also require the annotators to
      distinguish between supporting and opposing arguments. The IAA, using
      Cohen's kappa metric, is 0.721 for two expert annotators and 0.40 for
      non-experts.
  - dest: "assets/guidelines/shnarch2018.pdf"
    url: "https://github.com/terne/Annotator-Bias-in-Argmin/blob/main/guidelines/guideline_four.pdf"
    description: >-
      Annotation guidelines from [Shnarch et al.
      (2018)](https://aclanthology.org/2020.findings-emnlp.243/) for
      context-dependent claim and premise detection. They use the term claim as
      meaning the conclusion and premise as a type of evidence. The IAA, using
      Fleiss' kappa metric, is 0.45.

# Workflows are sequences of commands (see below) executed in order. You can
# run them via "spacy project run [workflow]". If a commands's inputs/outputs
# haven't changed, it won't be re-run.
workflows:
  all:
    - convert
    - fetch
    - evaluate

# Project commands, specified in a style similar to CI config files (e.g. Azure
# pipelines). The name is the command name that lets you trigger the command
# via "spacy project run [command] [path]". The help message is optional and
# shown when executing "spacy project run [optional command] [path] --help".
commands:
  - name: "convert"
    help: "Convert UKP's TSV file into both JSONL and spaCy formats"
    script:
      - mkdir -p corpus/jsonl/${vars.source_name}/
      - python -m scripts.convert assets/${vars.source_name}.tsv corpus/jsonl/${vars.source_name}/ --jsonl
      - mkdir -p corpus/spacy/${vars.source_name}/
      - python -m scripts.convert assets/${vars.source_name}.tsv corpus/spacy/${vars.source_name}/
    deps:
      - assets/${vars.source_name}.tsv
    outputs:
      - corpus/jsonl/${vars.source_name}/train.jsonl
      - corpus/jsonl/${vars.source_name}/val.jsonl
      - corpus/jsonl/${vars.source_name}/test.jsonl
      - corpus/spacy/${vars.source_name}/train.jsonl
      - corpus/spacy/${vars.source_name}/val.jsonl
      - corpus/spacy/${vars.source_name}/test.jsonl

  - name: "fetch"
    help: "Run an LLM-assisted textcat.fetch recipe to label text in bulk"
    script:
      - >-
        python -m prodigy langchain.textcat.fetch
        corpus/jsonl/${vars.source_name}/${vars.dataset}.jsonl outputs/langchain-${vars.dataset}-${vars.annotation_guideline}.jsonl
        --annotation-guideline assets/guidelines/${vars.annotation_guideline}.pdf
        -F scripts/recipes/textcat.py
    deps:
      - corpus/jsonl/${vars.source_name}/${vars.dataset}.jsonl
      - assets/guidelines/${vars.annotation_guideline}.pdf
    outputs:
      - outputs/langchain-${vars.dataset}-${vars.annotation_guideline}.jsonl

  - name: "evaluate"
    help: "Evaluate zero-shot predictions on a gold-annotated dataset"
    script:
      - >-
        python -m scripts.evaluate corpus/spacy/${vars.source_name}/${vars.dataset}.spacy outputs/langchain-${vars.dataset}-${vars.annotation_guideline}.jsonl
        --output-path metrics/langchain-${vars.dataset}-${vars.annotation_guideline}.jsonl
    deps:
      - corpus/spacy/${vars.source_name}/${vars.dataset}.spacy
      - outputs/langchain-${vars.dataset}-${vars.annotation_guideline}.jsonl
    outputs:
      - metrics/langchain-${vars.dataset}-${vars.annotation_guideline}.jsonl
