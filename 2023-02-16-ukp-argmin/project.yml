title: "GPT-3 annotation for argument mining"
description: |
  Accompanying spaCy project for my blog post, [*GPT-3 for
  argument mining
  annotation*](https://ljvmiranda921.github.io/notebook/2023/03/28/chain-of-thought-annotation/), where
  I explored how LLM-assisted annotation can help in complex annotation tasks.

  I am using an argument mining dataset from the [UKP Sentential Argument Mining
  Corpus](https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/2345) ([Stab et
  al., 2018](https://aclanthology.org/D18-1402/)). Here, they have sentences on
  a variety of issues like cloning, minimum wage, abortion, with labels such as
  `NoArgument`, `Argument_For`, and `Argument_Against`. Note that you need to
  [send a request](https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/2345/restricted-resource?bitstreamId=90a1de18-7a2e-4706-89e6-cf8108cfd3e9)
  to the TU Datalib in order to access the corpus. Once you have the data, copy
  the `cloning.tsv` and `minimum_wage.tsv` into the `assets/` directory.

  The experiments here simply tests the "reliability" of zero-shot and
  chain-of-thought annotations from GPT-3. Ideally, there would be an
  HCI-element in my experiments but for the sake of the blog post, I'm limiting
  my work to empirical tests. However, if this work piqued your interest and
  you're working in HCI, let me know and we can collaborate!

  ### Getting the OpenAI annotations

  Make sure to [install Prodigy](https://prodi.gy/docs/install) as well as a few additional Python dependencies:

  ```bash
  python -m pip install prodigy -f https://XXXX-XXXX-XXXX-XXXX@download.prodi.gy
  python -m pip install -r requirements.txt
  ```

  With `XXXX-XXXX-XXXX-XXXX` being your personal Prodigy license key.

  Then, create a new API key from [openai.com](https://beta.openai.com/account/api-keys) or fetch an existing
  one. Record the secret key as well as the [organization key](https://beta.openai.com/account/org-settings)
  and make sure these are available as environmental variables. For instance, set them in a `.env` file in the
  root directory:

  ```
  OPENAI_ORG = "org-..."
  OPENAI_KEY = "sk-..."
  ```

  From there, you can run the following command to get batch annotations from OpenAI:

  ```
  python -m spacy project run openai
  ```

# These are the directories that the project needs. The project CLI will make
# sure that they always exist.
directories:
  - "assets"
  - "configs"
  - "corpus"
  - "scripts"
  - "training"
  - "metrics"

# Variables can be referenced across the project.yml using ${vars.var_name}
vars:
  config: "textcat.cfg"
  ukp_data: "minimum_wage.tsv"
  gpu_id: 0
  prompt_template: "openai_zero_shot.jinja2"

# Workflows are sequences of commands (see below) executed in order. You can
# run them via "spacy project run [workflow]". If a commands's inputs/outputs
# haven't changed, it won't be re-run.
workflows:
  textcat:
    - preprocess
    - train
    - evaluate
  openai:
    - openai-preprocess
    - openai-fetch-textcat
    - openai-evaluate

# Project commands, specified in a style similar to CI config files (e.g. Azure
# pipelines). The name is the command name that lets you trigger the command
# via "spacy project run [command] [path]". The help message is optional and
# shown when executing "spacy project run [optional command] [path] --help".
commands:
  - name: "preprocess"
    help: "Convert the data to spaCy's binary format"
    script:
      - "python -m scripts.convert_corpus assets/${vars.ukp_data}"
    deps:
      - "assets/${vars.ukp_data}"
    outputs:
      - "corpus/train.spacy"
      - "corpus/val.spacy"
      - "corpus/test.spacy"

  - name: "train"
    help: "Train a text classification model"
    script:
      - >-
        python -m spacy train configs/${vars.config}
        --paths.train corpus/train.spacy
        --paths.dev corpus/val.spacy
        --gpu-id ${vars.gpu_id}
        --output training/
    deps:
      - "corpus/train.spacy"
      - "corpus/val.spacy"
    outputs:
      - "training/model-best"

  - name: "evaluate"
    help: "Evaluate the model and export metrics"
    script:
      - >-
        python -m spacy evaluate training/model-best corpus/test.spacy
        --output metrics/scores.json
        --gpu-id ${vars.gpu_id}
    deps:
      - "training/model-best"
      - "corpus/test.spacy"
    outputs:
      - "metrics/scores.json"

  - name: "openai-preprocess"
    help: "Convert the corpus into JSONL files to load into Prodigy"
    script:
      - >-
        python -m scripts.convert_to_jsonl assets/${vars.ukp_data} corpus/test_data.jsonl
        --dataset test
    deps:
      - "assets/${vars.ukp_data}"
    outputs:
      - "corpus/test_data.jsonl"

  - name: "openai-fetch-textcat"
    help: "Run batch annotations using the `textcat.openai.fetch` recipe"
    script:
      - >-
        python -m prodigy textcat.openai.fetch corpus/test_data.jsonl corpus/openai_output.jsonl
        --prompt-path scripts/recipes/openai_templates/${vars.prompt_template}
        --labels Argument_for,Argument_against,NoArgument
        -F ./scripts/recipes/textcat.py
    deps:
      - "corpus/test_data.jsonl"
    outputs:
      - "corpus/openai_output.spacy"

  - name: "openai-evaluate"
    help: "Evaluate OpenAI annotations to the test data"
    script:
      - >-
        python -m scripts.evaluate_gpt corpus/openai_output.jsonl corpus/test.spacy
        --output metrics/scores_openai.json
    deps:
      - "corpus/openai_output.jsonl"
      - "corpus/test.spacy"
    outputs:
      - "metrics/scores_openai.json"

  - name: "openai-fetch-spans"
    help: "Run batch annotations using the `spans.openai.fetch` recipe"
