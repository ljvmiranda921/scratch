title: "Examining contrast pairs in datasets"
description: |
  I'm curious if lexical-based distances (e.g., get the word embeddings and then
  cosine distance) correlate with quality-based distances (e.g., rank distance)
  in preference data. My hypothesis is that they are not correlated in some
  domains like OpenQA, but they *can* be correlated in some like summarization
  or coding.

  | Domain        | Source                                                                                                                                               |
  |---------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
  | Summarization | [Open AI's Summarize from Human Feedback dataset](https://huggingface.co/datasets/openai/summarize_from_feedback)                                    |
  | OpenQA        | [Stanford Human Preferences Dataset (SHP)](https://huggingface.co/datasets/stanfordnlp/SHP)                                                          |
  | ClosedQA      | [Argilla's Cleaned Ultrafeedback, Flan-v2 subset](https://huggingface.co/datasets/argilla/ultrafeedback-multi-binarized-quality-preferences-cleaned) |
  | Mixed         | [AlpacaFarm Human Preferences](https://huggingface.co/datasets/tatsu-lab/alpaca_farm/viewer/alpaca_human_preference)                                 |

  Then, for each prompt-preference pair (x, y_w, y_l), I computed two distance metrics:
  * The **lexical distance**, which is the cosine distance of the BERT
    embeddings of the chosen and rejected response, including the prompt (x +
    y_w, x + y_l).  
  * The **quality distance**, which differs based on the dataset. For datasets with
    ordinal ranking, I computed for the linear difference between ranks. 

  Finally, I measured the Pearson correlation between the two distance measures.
  Below are my findings:

directories:
  - "scripts"
  - "embeddings"
  - "outputs"

commands:
  - name: "embed-dataset"
    help: "Embed dataset using an embedding model."
    script:
      - python3 -m scripts.embed_dataset openai/summarize_from_feedback embeddings
      - python3 -m scripts.embed_dataset stanford/SHP embeddings
      - python3 -m scripts.embed_dataset argilla/ultrafeedback-multi-binarized-quality-preferences-cleaned embeddings
      - python3 -m scripts.embed_dataset tatsu-lab/alpaca_farm embeddings
    outputs:
      - embeddings

  - name: "get-correlation"
    help: "Compute Pearson correlation for each task."
    script:
      - ls

  - name: "visualize-embeddings"
    help: "Visualize the chosen and rejected pairs for each task."
    script:
      - python3 -m scripts.visualize_embeddings embeddings outputs
