title: "Examining contrast pairs in datasets"
description: |
  I'm curious if lexical-based distances (e.g., get the word embeddings and then
  cosine distance) correlate with quality-based distances (e.g., rank distance)
  in preference data. My hypothesis is that they are not correlated in some
  domains like OpenQA, but they *can* be correlated in some like summarization
  or coding.

directories:
  - "scripts"
  - "embeddings"
  - "outputs"

commands:
  - name: "get-dist-histogram"
    help: "Visualize cosine distances between preference pairs from an embedding model."
    script:
      - python3 -m scripts.embed_dataset openai/summarize_from_feedback embeddings/get-dist-histogram/
      - python3 -m scripts.embed_dataset stanford/SHP embeddings/get-dist-histogram/
      - python3 -m scripts.embed_dataset argilla/ultrafeedback-multi-binarized-quality-preferences-cleaned embeddings
      - python3 -m scripts.embed_dataset tatsu-lab/alpaca_farm embeddings/general
      - python3 -m scripts.embed_dataset berkeley-nest/Nectar embeddings/get-dist-histogram/
      - python3 -m scripts.visualize_distances embeddings/get-dist-histogram/ outputs/
    outputs:
      - embeddings/get-dist-histogram/
      - outputs/distance_hist_plot.html

  - name: "get-dist-ranking"
    help: "Run experiment to get cosine distances for each rank"
    script:
      - python3 -m scripts.experiment_embed_dataset_per_rank embed
      - python3 -m scripts.experiment_embed_dataset_per_rank visualize
    outputs:
      - embeddings/get-dist-ranking/
      - outputs/distance_hist_ranking_plot.html

  - name: "get-pearson-correlation"
    help: "Run experiment to get pearson correlation between distances"
    script:
      - python3 -m scripts.experiment_embed_dataset_per_rank embed
      - python3 -m scripts.experiment_embed_dataset_per_rank compute-correlation
